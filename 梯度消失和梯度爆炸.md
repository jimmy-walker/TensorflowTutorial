# 梯度消失和梯度爆炸

层数比较多的神经网络模型在训练时也是会出现一些问题的，其中就包括梯度消失问题（gradient vanishing problem）和梯度爆炸问题（gradient exploding problem）。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。

例如，对于下图所示的含有3个隐藏层的神经网络，梯度消失问题发生时，接近于输出层的hidden layer 3等的权值更新相对正常，但前面的hidden layer 1的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做了一个同一映射，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。

![img](https://pic3.zhimg.com/80/v2-82873a89ff3c14c1d3b42d1862917f35_hd.jpg)

而这种问题为何会产生呢？以下图的反向传播为例（假设每一层只有一个神经元且对于每一层$y_i=\sigma\left(z_i\right)=\sigma\left(w_ix_i+b_i\right)$，其中$\sigma$为sigmoid函数）

![img](https://pic3.zhimg.com/80/v2-b9e0d6871fbcae05d602bab65620a3ca_hd.jpg)

可以推导出
$$
\begin{align}
&\frac{\partial C}{\partial b_1}=\frac{\partial C}{\partial y_4}\frac{\partial y_4}{\partial z_4}\frac{\partial z_4}{\partial x_4}\frac{\partial x_4}{\partial z_3}\frac{\partial z_3}{\partial x_3}\frac{\partial x_3}{\partial z_2}\frac{\partial z_2}{\partial x_2}\frac{\partial x_2}{\partial z_1}\frac{\partial z_1}{\partial b_1}\\
&=\frac{\partial C}{\partial y_4}\sigma'\left(z_4\right)w_4\sigma'\left(z_3\right)w_3\sigma'\left(z_2\right)w_2\sigma'\left(z_1\right)
\end{align}
$$

而sigmoid的函数及其导数如下图

![](picture/sigmoid.jpg)
$$
\begin{aligned}
f'(z) &= (\frac{1}{1+e^{-z}})' 
\\
&= \frac{e^{-z}}{(1+e^{-z})^{2}} 
\\
&= \frac{1+e^{-z}-1}{(1+e^{-z})^{2}}  
\\
&= \frac{1}{(1+e^{-z})}(1-\frac{1}{(1+e^{-z})}) 
\\
&= f(z)(1-f(z))
\\
\end{aligned}
$$


![img](https://pic4.zhimg.com/80/v2-da5606a2eebd4d9b6ac4095b398dacf5_hd.jpg)

~~可见，$\sigma'\left(x\right)$的最大值为$\frac{1}{4}$，而我们初始化的网络权值通常都小于1（正态分布，均匀分布，Xavier等的初始化一般大概率都是小于1的），因此$|\sigma'\left(z\right)w|\leq\frac{1}{4}$，因此对于上面的链式求导，层数越多，求导结果$\frac{\partial C}{\partial b_1}$越小，因而导致梯度消失的情况出现。~~

**J也可以理解为两端的导数接近为0，意味着$z$很大或很小时，$\frac{\partial y}{\partial z}$就接近于0，因此整个式子$\frac{\partial C}{\partial b_1}$就接近于0。**

这样，梯度爆炸问题的出现原因就显而易见了，即$|\sigma'\left(z\right)w|>1$，也就是$w$比较大的情况。但对于使用sigmoid激活函数来说，这种情况比较少。因为$\sigma'\left(z\right)$的大小也与$w$有关（$z=wx+b$），除非该层的输入值$x$一直在一个比较小的范围内。

**<u>J也可以理解由sigmoid图像可知，为z需要在很小的范围内（上图当中部分），才能保证导数值很大，在w，b固定情况下，需要x在一个比较小的变化范围内。因此，参数所在的范围是比较窄的。</u>**

## 应对方法

1. 使用Xavier或He参数初始化策略；
2. 使用ELU或ReLU及其变种激活函数。

## Xavier and He initialization

## Relu

因为relu函数在正值区域的导数为常数，所以不会有抱和问题。但是会导致[dying Relus](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)的问题。

![](picture/relu derivate.jpg)

# Reference

- [神经网络训练中的梯度消失与梯度爆炸](https://zhuanlan.zhihu.com/p/25631496)